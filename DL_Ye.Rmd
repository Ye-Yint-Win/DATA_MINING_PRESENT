---
title: "DL_ye"
author: "Ye"
date: "2024-03-26"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Linear Regression and Data Manipulation
library(ISLR2)
library(MASS)
library(caret)
library(dslabs)
library(mlbench)
library(data.table)

# Neural Network Modeling
library(keras)
library(tensorflow)
library(reticulate)

# Image Classification with MNIST
library(mnist)
library(ggplot2)
library(tidyverse)
library(glmnet)
```

Let's start by preparing the data by splitting with testid

```{r}
# Remove rows with missing values from the dataset
Gitters <- na.omit(Hitters)

n <- nrow(Gitters)
set.seed(13)
ntest <- n/3

# Randomly sample row indices to create the test dataset
testid <- sample(1:n, ntest)
```

Fitting the model into a linear model

```{r}
lfit <- lm(Salary ~ ., data = Gitters[-testid , ])

#Prediction
lpred <- predict(lfit, Gitters[testid , ])

#MEan Absolute Prediction error between train and test data.
with(Gitters[testid , ], mean(abs(lpred - Salary)))
```

Scaling the Response Varaible and create the response variable This
function scale() automatically converts factors to dummy variables. The
scale() function standardizes the matrix so each column has mean zero
and variance one.

Cross-validated lasso regression model using cv.glmnet Getting MAE from
lasso regression model

```{r}
#Scaling the predictors 
x <- scale(model.matrix(Salary ~ . - 1, data = Gitters))
y <- Gitters$Salary

#Fitting into Lasso regression
cvfit <- cv.glmnet(x[-testid , ], y[-testid],type.measure = "mae")

#Prediction
cpred <- predict(cvfit , x[testid , ])

#MAE
mean(abs(y[testid] - cpred))
```

## Single Layer Neural Network

ReLU model(Rectified Linear Model) $$
f(x) = \begin{cases} 
0 & \text{if } x \leq 0 \\
x & \text{if } x > 0 
\end{cases}
$$

To fit the neural network, we have to set up a model structure that
describe the network.

We now return to our neural network. The object modnn has a single
hidden layer with 50 hidden units, and a ReLU activation function. It
then has a dropout layer, in which a random 40% of the 50 activations
from the previous layer are set to zero during each iteration of the
stochastic gradient descent algorithm. Finally, the output layer has
just one unit with no activation function, indicating that the model
provides a single quantitative output

keras_model_sequential(): This function initializes an empty sequential
model, which is a linear stack of layers. layer_dense: This function
adds a fully connected (dense) layer to the neural network.

layer_dropout : This function adds a dropout layer to the neural
network. Dropout is a regularization technique used to prevent
overfitting by randomly setting a fraction of input units to zero during
training. 
layer_dense: output layer for linear regression. It connects
the layer with a single neuron, which is typical for linear regression.

```{r}
modnn <- keras_model_sequential() %>%
  layer_dense(units = 50, activation = "sigmoid",input_shape = ncol(x)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1)


x <- model.matrix(Salary ~ . - 1, data = Gitters) %>% scale()
modnn
```

Compile(): this sets up the neural network model to use MSE as loss
funciton, RMSprop as optimizer annd MSE as metric for performance.

Optimizer is an optimization algorithm that adjust the learning rate for
each parameter based on the average fo recent gradients.

Then, we fit the model into a plot.

Epochs: the number of epochs or iterations over the entire dataset
during the training. 
batch_size: the algorithm randomly select this
amount of training observations for the computation of the gradient(how
much we need to adjust).

```{r}
modnn %>% compile(
  loss = "mse",
  optimizer = optimizer_rmsprop(),
  metrics = list("mean_absolute_error")
)

history <- modnn %>% 
  fit(
    x[-testid , ], y[-testid], epochs = 400, batch_size = 32, validation_data = list(x[testid , ], y[testid])
    )
```

Now, we predict the values based on the model and get the MAE of the
model. Then, We plot.

```{r}
npred <- predict(modnn , x[testid , ])
mean(abs(y[testid] - npred))

plot(history)
```

## Multi-Layer Neural Networks

We'll use MNIST dataset provided in the textbook.

This dataset is 60,000 images in the training data and 10,000 in the
test data. The images are 28Ã—28, and stored as a three-dimensional
array, so we need to reshape them into a matrix.

```{r}
mnist_file <- "/Users/yeyintwin/Desktop/DM_assign/mnist.npz"

# Load the MNIST dataset from the local file
mnist <- keras::dataset_mnist(path = mnist_file)
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y

dim(train_images)
dim(test_images)

train_images <- array_reshape(train_images, c(nrow(train_images), 784))
test_images <- array_reshape(test_images, c(nrow(test_images), 784))
train_labels <- to_categorical(train_labels, 10)
test_labels <- to_categorical(test_labels, 10)

train_images <- train_images / 255
test_images <- test_images / 255

modelnn <- keras_model_sequential()
modelnn %>%
  layer_dense(units = 256, activation = "relu",
              input_shape = c(784)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = "softmax")
summary(modelnn)

modelnn %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(), 
  metrics = c("accuracy")
)

system.time(
  history <- modelnn %>%
    fit(train_images, train_labels, epochs = 20, batch_size = 128,
        validation_split = 0.2)
)


accuracy <- function(pred, truth) {
  mean(drop(as.numeric(pred)) == drop(truth))
}

modelnn %>% predict(test_images) %>% k_argmax() %>% accuracy(test_labels)

modellr <- keras_model_sequential() %>%
  layer_dense(input_shape = 784, units = 10,
              activation = "softmax")
summary(modellr)


#Generalization criterion for two-class logistic regression
modellr %>% compile(loss = "categorical_crossentropy",
                    optimizer = optimizer_rmsprop(), 
                    metrics = c("accuracy"))
modellr %>% fit(train_images, train_labels, epochs = 20,
                batch_size = 128, validation_split = 0.2)

modellr %>% predict(test_images) %>% k_argmax() %>% accuracy(test_labels)

```

```{r}
plot(history, smooth = TRUE)
```


```{r}
data(brca)
predictors = data.frame(brca$x)
brca_data = data.frame(x = predictors, 
                       y = fct_relevel(brca$y, c("M","B"))) 
set.seed(42)


n<- nrow(brca_data)

ntest<- trunc(n/3)
testid<- sample(1:n, ntest)
```

```{r}
mod.fit<- lm(x.radius_mean ~., data = brca_data[-testid , ])
mod.fit
```

```{r}
mod.pred <- predict(mod.fit,  brca_data[testid , ])

with(brca_data[testid , ], mean(abs(mod.pred- x.radius_mean)))
```

```{r}
x <- scale(model.matrix( x.radius_mean ~.-1, data = brca_data))
y <- brca_data$x.radius_mean

cvfit<- cv.glmnet(x[-testid, ], y[-testid], type.measure = "mae")
cvpred<- predict(cvfit, x[testid, ], s = "lambda.min")
mean(abs(y[testid]- cvpred))

```

```{r}
modnn <- keras_model_sequential() %>%
 layer_dense(units = 50, activation = "relu",
input_shape = ncol(x)) %>%
 layer_dropout(rate = 0.4) %>%
 layer_dense(units = 1)

x <- scale(model.matrix( x.radius_mean ~.-1, data = brca_data))
x <- scale(model.matrix( x.radius_mean ~.-1, data = brca_data))%>% scale()


modnn %>% compile(loss = "mse",
                  optimizer = optimizer_rmsprop(),
                  metrics = list("mean_absolute_error")
                  )
```

$$g(z) = $$

```{r}
history <- modnn %>% fit(
  x[-testid , ], y[-testid], epochs = 20, batch_size = 32,
  validation_data = list(x[testid , ], y[testid])
            )
```

```{r}
plot(history)
```

```{r}
npred <- predict(modnn , x[testid , ])
mean(abs(y[testid] - npred))
```


Multi-Layer Neural Network Homework
```{r}
# Load datasets
train <- read.csv("https://raw.githubusercontent.com/ben519/MLPB/master/Problems/Classify%20Images%20of%20Stairs/_Data/train.csv")
test <- read.csv("https://raw.githubusercontent.com/ben519/MLPB/master/Problems/Classify%20Images%20of%20Stairs/_Data/test.csv")
```


Using the train and test sets above can you begin reshaping the data so that we can begin defining the neural network?
Now, the next step is to preprocess the datasets.

```{r}
train_images <- array_reshape(as.matrix(train[, c("R1C1", "R1C2", "R2C1", "R2C2")]), c(nrow(train), 4))
test_images <- array_reshape(as.matrix(test[, c("R1C1", "R1C2", "R2C1", "R2C2")]), c(nrow(test), 4))
dim(train_images)
dim(test_images)
train_images <- train_images / 255
test_images <- test_images / 255
```

Question 2
Define the multi-layer neural network with activation layers of your choice.
```{r}

```

Question 3
It is important to complie the neural network before training it to get the optimal configuration that will help us train the model. Compile and then train the data set. Evaluate the model on the test data. Explain what you notice.
```{r}

```
Plot the model
```{r}

```

